model:
  name: "Qwen/Qwen3-Next-80B-A3B-Instruct"

  num_hidden_layers: 48
  hidden_size: 2048
  num_experts: 512
  num_experts_per_tok: 10
  max_position_embeddings: 262144 # 256k context!

  # increase later
  max_length: 2048

  # Special tokens from config
  pad_token_id: 151654
  bos_token_id: 151643
  eos_token_id: 151645

training:
  num_epochs: 3

  # Per-device batch size
  batch_size: 1

  # Gradient accumulation for effective batch size
  # Effective batch = 1 × 16 × 8 GPUs = 128
  gradient_accumulation_steps: 16

  # Optimizer settings
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0

  # Optimizer type
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8

fsdp:
  # From FSDPConfig dataclass
  sharding_strategy: "FULL_SHARD"

  # Mixed precision
  use_mixed_precision: true
  param_dtype: "bfloat16"
  reduce_dtype: "float32"
  buffer_dtype: "bfloat16"

  # Memory optimizations
  cpu_offload: false # Change later
  use_activation_checkpointing: true
  use_gradient_checkpointing: true

  # Performance
  backward_prefetch: "BACKWARD_PRE"
  forward_prefetch: true
  limit_all_gathers: true

  # FSDP settings
  sync_module_states: true
  use_orig_params: true

logging:
  # How often to log metrics
  log_every_n_steps: 10

  # Profiling frequency
  profile_every_n_steps: 100
  profile_enabled: true

  # Checkpointing
  save_every_n_steps: 500
  save_total_limit: 3 # Keep only 3 checkpoints

  # WandB settings
  wandb:
    project: "manimbot"
    name: "qwen3next-80b-manim-fsdp"
    tags: ["qwen3-next", "80b", "moe", "fsdp", "manim"]
    log_model: false # Set true to upload checkpoints to wandb

data:
  # Reuse existing dataloader
  streaming: true
  num_workers: 0 # Must be 0 for streaming
  pin_memory: true

  # Datasets (already configured in data_loader.py)
  datasets:
    - "generaleoley/manim-codegen"
    - "bespokelabs/bespoke-manim"
    - "thanhkt/manim_code"
    - "Edoh/manim_python"
